{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test BoW and GloVe methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from scipy.sparse import vstack\n",
    "import joblib \n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "method = \"BoW\" # BoW or GloVe\n",
    "input_path = '../data/processed/train_full_policy5.csv'\n",
    "glove_path = '../src/models/glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "hparams_tuning = False # True or False\n",
    "train_type = \"full\" # full or val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings from a file and returns a dictionary of word vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the GloVe embeddings file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where the keys are words and the values are the corresponding word vectors.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def tweet_to_glove_vector(tweet, embeddings, vector_size=200):\n",
    "    \"\"\"\n",
    "    Converts a tweet to a GloVe vector by averaging the vectors of the words in the tweet.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tweet : str\n",
    "        The tweet to be converted into a vector.\n",
    "    embeddings : dict\n",
    "        A dictionary of word vectors.\n",
    "    vector_size : int, optional\n",
    "        The size of the word vectors, by default 200.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A numpy array representing the averaged vector of the tweet.\n",
    "    \"\"\"\n",
    "    words = tweet.lower().split()\n",
    "    tweet_vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in embeddings:\n",
    "            tweet_vec += embeddings[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        tweet_vec /= count\n",
    "    return tweet_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "# Remove NaNs due to pre-processing pipeline\n",
    "df = df.dropna(subset=['tweet'])\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Check the mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_mapping)\n",
    "\n",
    "X = df['tweet']\n",
    "y = df['label']\n",
    "\n",
    "if hparams_tuning == False:\n",
    "    if train_type == \"val\":\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "        if method==\"BoW\":\n",
    "            # Bag of Words embeddings\n",
    "            vectorizer = CountVectorizer(max_features=5000)\n",
    "            X_train = vectorizer.fit_transform(X_train)\n",
    "            X_val = vectorizer.transform(X_val)\n",
    "            \n",
    "        elif method==\"GloVe\":\n",
    "            # GloVe embeddings\n",
    "            glove_embeddings  = load_glove_embeddings(glove_path)\n",
    "            print(f\"Loaded {len(glove_embeddings )} word vectors.\")\n",
    "            X_train = np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X_train])\n",
    "            X_val= np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X_val])\n",
    "            \n",
    "    elif train_type == \"full\":\n",
    "        if method==\"BoW\":\n",
    "            # Bag of Words embeddings\n",
    "            vectorizer = CountVectorizer(max_features=5000)\n",
    "            X = vectorizer.fit_transform(X)\n",
    "            \n",
    "        elif method==\"GloVe\":\n",
    "            # GloVe embeddings\n",
    "            glove_embeddings  = load_glove_embeddings(glove_path)\n",
    "            print(f\"Loaded {len(glove_embeddings )} word vectors.\")\n",
    "            X = np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X])\n",
    "        \n",
    "else:\n",
    "    # No splitting into X_train and X_val as we are going to perform hparams tuning with k-fold cross-validation\n",
    "    if method==\"BoW\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "        X = vectorizer.fit_transform(X)\n",
    "        \n",
    "    elif method==\"GloVe\":\n",
    "        glove_embeddings  = load_glove_embeddings(glove_path)\n",
    "        print(f\"Loaded {len(glove_embeddings )} word vectors.\")\n",
    "        X = np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hparams_tuning == True:\n",
    "    # Hyper-parameters tuning with k-fold cross-validation using GridSearchCV\n",
    "    \n",
    "    # Models definition\n",
    "    models = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    LinearSVC(random_state=random_state),\n",
    "    RidgeClassifier(random_state=random_state),\n",
    "    SGDClassifier(random_state=random_state),\n",
    "    ExtraTreesClassifier(random_state=random_state),\n",
    "    MLPClassifier(verbose=False, random_state=random_state)\n",
    "    ]\n",
    "    models_names = [\n",
    "        'Logistic Regression',\n",
    "        'Support Vector Machine',\n",
    "        'Ridge Classifier',\n",
    "        'SGD Classifier',\n",
    "        'Extra Trees',\n",
    "        'Multi Layer Perceptron',\n",
    "    ]\n",
    "    \n",
    "    # Models hparams\n",
    "    models_hparams = [\n",
    "    # LogisticRegression\n",
    "    {'C': [0.01, 0.1, 1, 10], 'solver': ['lbfgs', 'saga']},\n",
    "    \n",
    "    # LinearSVC\n",
    "    {'C': [0.01, 0.1, 1, 10], 'loss': ['hinge', 'squared_hinge']},\n",
    "    \n",
    "    # RidgeClassifier\n",
    "    {'alpha': [0.01, 0.1, 1, 10]},\n",
    "    \n",
    "    # SGDClassifier\n",
    "    {'loss': ['hinge', 'log_loss'], 'alpha': [0.0001, 0.01, 0.1], 'penalty': ['l2', 'l1', 'elasticnet']},\n",
    "    \n",
    "    # ExtraTreesClassifier\n",
    "    {'n_estimators': [50, 100], 'min_samples_split': [2, 5, 10], 'criterion': ['gini']},\n",
    "    \n",
    "    # MLPClassifier\n",
    "    {'hidden_layer_sizes': [(50), (100), (50, 50)], 'activation': ['relu'], 'solver': ['adam'], 'alpha': [0.001, 0.01, 1]}\n",
    "    ]\n",
    "    \n",
    "    chosen_hparams = list()\n",
    "    estimators = list()\n",
    "    results = list()\n",
    "    \n",
    "    # Training loop\n",
    "    for model, model_name, hparams in zip(models, models_names, models_hparams):\n",
    "        \n",
    "            print(\"\\n########       {}       ########\".format(model_name))\n",
    "            starting_time = time.time()\n",
    "            clf = GridSearchCV(estimator=model, param_grid=hparams, scoring='accuracy', cv=5)\n",
    "            clf.fit(X, y)\n",
    "            ending_time = time.time()\n",
    "            chosen_hparams.append(clf.best_params_)\n",
    "            estimators.append((model_name, clf.best_score_, clf.best_estimator_))\n",
    "            \n",
    "            for hparam in hparams:\n",
    "                print(f'\\t--> best value for hyperparameter \"{hparam}\": ', clf.best_params_.get(hparam))\n",
    "            \n",
    "            mean_accuracy = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "            std_score = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "            \n",
    "            # Save models with repsective accuracy\n",
    "            results.append((model_name, model, mean_accuracy, std_score))\n",
    "        \n",
    "            print(f'\\t--> best model mean accuracy:', mean_accuracy)\n",
    "            print(f'\\t--> best model std:', std_score)\n",
    "            print(f'\\tElapsed time for GridSearch: ', timedelta(seconds=ending_time - starting_time))\n",
    "            \n",
    "    # Find the best model based on accuracy\n",
    "    best_model_name, best_model, best_accuracy, _ = max(results, key=lambda x: x[2])\n",
    "    print(f\"\\nBest Model: {best_model_name} with accuracy {best_accuracy}\")\n",
    "            \n",
    "elif train_type == \"val\":\n",
    "    # Training models with best hyper-parameters\n",
    "    \n",
    "    # Models definition\n",
    "    models = [\n",
    "    LogisticRegression(random_state=random_state, C=1, solver='saga'),\n",
    "    LinearSVC(random_state=random_state, C=0.1, loss='squared_hinge'),\n",
    "    RidgeClassifier(random_state=random_state, alpha=10),\n",
    "    SGDClassifier(random_state=random_state, loss='hinge', alpha=0.0001, penalty='l2'),\n",
    "    ExtraTreesClassifier(random_state=random_state, n_estimators=100, min_samples_split=5, criterion='gini'),\n",
    "    MLPClassifier(verbose=False, random_state=random_state, hidden_layer_sizes=(100), activation='relu', solver='adam', alpha=0.001)\n",
    "    ]\n",
    "    models_names = [\n",
    "        'Logistic Regression',\n",
    "        'Support Vector Machine',\n",
    "        'Ridge Classifier',\n",
    "        'SGD Classifier',\n",
    "        'Extra Trees',\n",
    "        'Multi Layer Perceptron',\n",
    "    ]\n",
    "\n",
    "    results = list()\n",
    "    \n",
    "    # Training loop\n",
    "    for model, model_name in zip(models, models_names):\n",
    "        print(f\"\\n########       {model_name}       ########\")\n",
    "        starting_time = time.time()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        ending_time = time.time()\n",
    "        print(f'Elapsed time: {timedelta(seconds=ending_time - starting_time)}')\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        # Save models with repsective accuracy\n",
    "        results.append((model_name, model, accuracy))\n",
    "        \n",
    "    # Find the best model based on accuracy\n",
    "    best_model_name, best_model, best_accuracy = max(results, key=lambda x: x[2])\n",
    "    print(f\"\\nBest Model: {best_model_name} with accuracy {best_accuracy}\")\n",
    "\n",
    "    # Combine the training and validation sets for final training\n",
    "    X = vstack([X_train, X_val])\n",
    "    y = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    # Final training on entire data\n",
    "    best_model.fit(X, y)\n",
    "    # Save the trained best model to disk\n",
    "    joblib.dump(model, f'../models/model_{method}_{best_model_name}.pkl')\n",
    "    \n",
    "elif train_type == \"full\":\n",
    "    # Models definition\n",
    "    models = [\n",
    "    LogisticRegression(random_state=random_state, C=1, solver='saga'),\n",
    "    LinearSVC(random_state=random_state, C=0.1, loss='squared_hinge'),\n",
    "    RidgeClassifier(random_state=random_state, alpha=10),\n",
    "    SGDClassifier(random_state=random_state, loss='hinge', alpha=0.0001, penalty='l2'),\n",
    "    ExtraTreesClassifier(random_state=random_state, n_estimators=100, min_samples_split=5, criterion='gini'),\n",
    "    MLPClassifier(verbose=False, random_state=random_state, hidden_layer_sizes=(100), activation='relu', solver='adam', alpha=0.001)\n",
    "    ]\n",
    "    models_names = [\n",
    "        'Logistic_Regression',\n",
    "        'Support_Vector_Machine',\n",
    "        'Ridge_Classifier',\n",
    "        'SGD_Classifier',\n",
    "        'Extra_Trees',\n",
    "        'Multi_Layer_Perceptron',\n",
    "    ]\n",
    "\n",
    "    # Final training on entire data\n",
    "    for model, model_name in zip(models, models_names):\n",
    "        print(f\"\\n########       Full training: {model_name}       ########\")\n",
    "        starting_time = time.time()\n",
    "        model.fit(X, y)\n",
    "        ending_time = time.time()\n",
    "        print(f'Elapsed time: {timedelta(seconds=ending_time - starting_time)}')\n",
    "        \n",
    "        # Save the trained model to disk\n",
    "        joblib.dump(model, f'../models/model_{method}_{model_name}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "df_test = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "best_model = joblib.load('../models/model.pkl')\n",
    "X_test = df_test['tweet']\n",
    "\n",
    "# Transform the test set using the same vectorizer and make predictions\n",
    "if method == \"BoW\":\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "elif method == \"GloVe\":\n",
    "    X_test_vec= np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X_test])\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "# Create the final DataFrame with Id and Prediction columns\n",
    "df_test['prediction'] = y_test_pred\n",
    "df_test['prediction'] = df_test['prediction'].replace(0, -1)\n",
    "df_final = df_test[['id', 'prediction']]\n",
    "df_final = df_final.rename(columns={'id': 'Id', 'prediction': 'Prediction'})\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "df_final.to_csv(f'../results/predictions_{method}.csv', index=False)\n",
    "print(df_final.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
