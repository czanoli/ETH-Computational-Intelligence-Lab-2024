{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from scipy.sparse import vstack\n",
    "import joblib \n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def tweet_to_glove_vector(tweet, embeddings, vector_size=200):\n",
    "    words = tweet.lower().split()\n",
    "    tweet_vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in embeddings:\n",
    "            tweet_vec += embeddings[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        tweet_vec /= count\n",
    "    return tweet_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"GloVe\" # BoW GloVe\n",
    "input_path = '../data/processed/train_full.csv'\n",
    "glove_path = '../src/models/glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "hparams_tuning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet     label\n",
      "0  instrumental clinical phonetics ( exc business...  negative\n",
      "1  going public ( mlps originally released in 197...  negative\n",
      "2  hahaha and he was not saw if he should were bl...  positive\n",
      "3        happy national high five day from pam & jim  positive\n",
      "4                          probs the skirt in my avi  positive\n",
      "{'negative': 0, 'positive': 1}\n",
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "# OCHO MI DA ERRORE PER DEI NAN (causa pre-processing, la parte di cleaning)\n",
    "df = df.dropna(subset=['tweet'])\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Check the mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_mapping)\n",
    "\n",
    "X = df['tweet']\n",
    "y = df['label']\n",
    "\n",
    "if hparams_tuning == False:\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "    if method==\"BoW\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_val = vectorizer.transform(X_val)\n",
    "        \n",
    "    elif method==\"GloVe\":\n",
    "        glove_embeddings  = load_glove_embeddings(glove_path)\n",
    "        print(f\"Loaded {len(glove_embeddings )} word vectors.\")\n",
    "        X_train = np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X_train])\n",
    "        X_val= np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X_val])\n",
    "else:\n",
    "    if method==\"BoW\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "        X = vectorizer.fit_transform(X)\n",
    "        \n",
    "    elif method==\"GloVe\":\n",
    "        glove_embeddings  = load_glove_embeddings(glove_path)\n",
    "        print(f\"Loaded {len(glove_embeddings )} word vectors.\")\n",
    "        X = np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    LinearSVC(random_state=random_state),\n",
    "    RidgeClassifier(random_state=random_state),\n",
    "    SGDClassifier(random_state=random_state),\n",
    "    ExtraTreesClassifier(random_state=random_state),\n",
    "    MLPClassifier(verbose=False, random_state=random_state),\n",
    "]\n",
    "\n",
    "         \n",
    "models_names = [\n",
    "    'Logistic Regression',\n",
    "    'Support Vector Machine',\n",
    "    'Ridge Classifier',\n",
    "    'SGD Classifier',\n",
    "    'Extra Trees',\n",
    "    'Multi Layer Perceptron',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########       Logistic Regression       ########\n"
     ]
    }
   ],
   "source": [
    "if hparams_tuning == True:\n",
    "    models_hparams = [\n",
    "    # LogisticRegression\n",
    "    {'C': [0.01, 0.1, 1, 10], 'solver': ['lbfgs', 'saga']},\n",
    "    \n",
    "    # LinearSVC\n",
    "    {'C': [0.01, 0.1, 1, 10], 'loss': ['hinge', 'squared_hinge']},\n",
    "    \n",
    "    # RidgeClassifier\n",
    "    {'alpha': [0.01, 0.1, 1, 10]},\n",
    "    \n",
    "    # SGDClassifier\n",
    "    {'loss': ['hinge', 'log'], 'alpha': [0.001, 0.01, 0.1], 'penalty': ['l2', 'l1', 'elasticnet']},\n",
    "    \n",
    "    # ExtraTreesClassifier\n",
    "    {'n_estimators': [100], 'min_samples_split': [2], 'criterion': ['gini']},\n",
    "    \n",
    "    # MLPClassifier\n",
    "    {'hidden_layer_sizes': [(50)], 'activation': ['relu'], 'solver': ['adam'], 'alpha': [0.1]}\n",
    "    ]\n",
    "    chosen_hparams = list()\n",
    "    estimators = list()\n",
    "    results = list()\n",
    "    for model, model_name, hparams in zip(models, models_names, models_hparams):\n",
    "        \n",
    "            print(\"\\n########       {}       ########\".format(model_name))\n",
    "            starting_time = time.time()\n",
    "            clf = GridSearchCV(estimator=model, param_grid=hparams, scoring='accuracy', cv=5)\n",
    "            clf.fit(X, y)\n",
    "            ending_time = time.time()\n",
    "            chosen_hparams.append(clf.best_params_)\n",
    "            estimators.append((model_name, clf.best_score_, clf.best_estimator_))\n",
    "            \n",
    "            for hparam in hparams:\n",
    "                print(f'\\t--> best value for hyperparameter \"{hparam}\": ', clf.best_params_.get(hparam))\n",
    "            \n",
    "            mean_accuracy = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "            std_score = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "            \n",
    "            # Save models with repsective accuracy\n",
    "            results.append((model_name, model, mean_accuracy, std_score))\n",
    "        \n",
    "            print(f'\\t--> best model mean accuracy:', mean_accuracy)\n",
    "            print(f'\\t--> best model std:', std_score)\n",
    "            print(f'\\tElapsed time for GridSearch: ', timedelta(seconds=ending_time - starting_time))\n",
    "            \n",
    "    # Find the best model based on accuracy\n",
    "    best_model_name, best_model, best_accuracy, _ = max(results, key=lambda x: x[2])\n",
    "    print(f\"\\nBest Model: {best_model_name} with accuracy {best_accuracy}\")\n",
    "            \n",
    "else:\n",
    "    results = list()\n",
    "    for model, model_name in zip(models, models_names):\n",
    "        print(f\"\\n########       {model_name}       ########\")\n",
    "        starting_time = time.time()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        ending_time = time.time()\n",
    "        print(f'Elapsed time: {timedelta(seconds=ending_time - starting_time)}')\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        # Save models with repsective accuracy\n",
    "        results.append((model_name, model, accuracy))\n",
    "        \n",
    "    # Find the best model based on accuracy\n",
    "    best_model_name, best_model, best_accuracy = max(results, key=lambda x: x[2])\n",
    "    print(f\"\\nBest Model: {best_model_name} with accuracy {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/model.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final training based on best model:\n",
    "if hparams_tuning == False:\n",
    "    # Combine the training and validation sets for final training\n",
    "    X = vstack([X_train, X_val])\n",
    "    y = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Final training on entire data\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(best_model, f'../models/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Prediction\n",
      "0   1          -1\n",
      "1   2          -1\n",
      "2   3          -1\n",
      "3   4           1\n",
      "4   5          -1\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "df_test = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "\n",
    "best_model = joblib.load('../models/model.pkl')\n",
    "X_test = df_test['tweet']\n",
    "\n",
    "# Transform the test set using the same vectorizer and make predictions\n",
    "if method == \"BoW\":\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "elif method == \"GloVe\":\n",
    "    X_test_vec= np.array([tweet_to_glove_vector(tweet, glove_embeddings) for tweet in X_test])\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "# Create the final DataFrame with Id and Prediction columns\n",
    "df_test['prediction'] = y_test_pred\n",
    "df_test['prediction'] = df_test['prediction'].replace(0, -1)\n",
    "df_final = df_test[['id', 'prediction']]\n",
    "df_final = df_final.rename(columns={'id': 'Id', 'prediction': 'Prediction'})\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "df_final.to_csv(f'../results/predictions.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the final DataFrame\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Stacking Classifier with the top 3 weak learners\n",
    "top_weak_learners = [(model_name, model) for model_name, model, _ in top_2_models]\n",
    "clf_stack = StackingClassifier(estimators = top_weak_learners, final_estimator = LogisticRegression())\n",
    "\n",
    "# Fit the StackingClassifier on the training data\n",
    "starting_time = time.time()\n",
    "clf_stack.fit(X_train, y_train)\n",
    "ending_time = time.time()\n",
    "print(f'Elapsed time: {timedelta(seconds=ending_time - starting_time)}')\n",
    "# Predict on validation set\n",
    "y_pred_stack = clf_stack.predict(X_val)\n",
    "# Calculate accuracy\n",
    "accuracy_stack = accuracy_score(y_val, y_pred_stack)\n",
    "# Add the StackingClassifier result\n",
    "results.append(('Stacking Classifier', clf_stack, accuracy_stack))\n",
    "print(f'\\nStacking Classifier Accuracy: {accuracy_stack}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on accuracy\n",
    "best_model_name, best_model, best_accuracy = max(results, key=lambda x: x[2])\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} with accuracy {best_accuracy}\")\n",
    "\n",
    "# Final training on entire data\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Save the trained model to disk\n",
    "import joblib\n",
    "joblib.dump(best_model, f'{method}_best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    LinearSVC(random_state=random_state),\n",
    "    #KNeighborsClassifier(n_jobs=-1),\n",
    "    MLPClassifier(verbose=False, random_state=random_state),\n",
    "    RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    AdaBoostClassifier(random_state=random_state),\n",
    "    ExtraTreesClassifier(random_state=random_state),\n",
    "    RidgeClassifier(random_state=random_state),\n",
    "    SGDClassifier(random_state=random_state),\n",
    "    GaussianNB(),\n",
    "    XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss'),\n",
    "    LGBMClassifier(random_state=random_state)\n",
    "]\n",
    "\n",
    "          \n",
    "models_names = [\n",
    "    'Logistic Regression',\n",
    "    'Support Vector Machine',\n",
    "    #'K Nearest Neighbors',\n",
    "    'Multi Layer Perceptron',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'AdaBoost',\n",
    "    'Extra Trees',\n",
    "    'Ridge Classifier',\n",
    "    'SGD Classifier',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'XGBoost',\n",
    "    'LightGBM'\n",
    "]\n",
    "\n",
    "\n",
    "models_hparams = [\n",
    "    {'solver': ['lbfgs'], 'penalty': ['l2'], 'C': [1e7], 'max_iter':[1000]},\n",
    "    #{'solver': ['saga'], 'penalty': ['l2'], 'C': [9e-2], 'fit_intercept':[True]},\n",
    "    \n",
    "    {'penalty': ['l2'], 'C': [3.75e-2], 'fit_intercept':[True]},\n",
    "    \n",
    "    #{'n_neighbors': [10], 'weights':['uniform']},\n",
    "    \n",
    "    {'hidden_layer_sizes': [(20)], 'max_iter': [100], 'activation': ['relu'], 'solver': ['adam'], 'alpha': [1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2, 0.3], 'n_estimators': [50, 100, 200], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5, 1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'alpha': [0.1, 1.0, 10.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
    "    \n",
    "    {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'], 'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {},  # GaussianNB has no hyperparameters to tune\n",
    "\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'num_leaves': [31, 50, 100], 'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [50, 100, 200], 'max_depth': [-1, 3, 5, 7, 10]}\n",
    "]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''chosen_hparams = list()\n",
    "estimators = list()\n",
    "\n",
    "for model, model_name, hparams in zip(models, models_names, models_hparams):\n",
    "    \n",
    "        print(\"\\n########       {}       ########\".format(model_name))\n",
    "        starting_time = time.time()\n",
    "        clf = GridSearchCV(estimator=model, param_grid=hparams, scoring='accuracy', cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        ending_time = time.time()\n",
    "        chosen_hparams.append(clf.best_params_)\n",
    "        estimators.append((model_name, clf.best_score_, clf.best_estimator_))\n",
    "        \n",
    "        for hparam in hparams:\n",
    "            print(f'\\t--> best value for hyperparameter \"{hparam}\": ', clf.best_params_.get(hparam))\n",
    "        \n",
    "        mean_test_score = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "        std_test_score = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "    \n",
    "        print(f'\\t--> best model mean accuracy:', mean_test_score)\n",
    "        print(f'\\t--> best model std:', std_test_score)\n",
    "        print(f'\\tElapsed time for GridSearch: ', timedelta(seconds=ending_time - starting_time))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ensemble: Stacking Classifier with top 3 weak learners\n",
    "\n",
    "# Sort estimators by the accuracy metric\n",
    "estimators.sort(key=lambda i:i[1],reverse=True)\n",
    "\n",
    "# Get the top 3 classifiers by their accuracy metric\n",
    "top3_clfs = list()\n",
    "for clf in estimators[0:3]:\n",
    "    top3_clfs.append((clf[0], clf[2]))\n",
    "    \n",
    "# Instantiate the Stacking Classifier with the top 3 weak learners\n",
    "clf_stack = StackingClassifier(estimators = top3_clfs, final_estimator = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_eval_estimators = list()\n",
    "for model_tuple in estimators:\n",
    "    model_name = model_tuple[0]\n",
    "    model = model_tuple[2]\n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, scoring=('accuracy'))\n",
    "    print('\\n')\n",
    "    print('The cross-validated Accuracy of {} is: '.format(model_name), np.mean(scores['test_score']))\n",
    "    perf_eval_estimators.append((model_name, np.mean(scores['test_score']), model))\n",
    "\n",
    "# Cross Validation for Stacking Ensemble\n",
    "scores = cross_validate(clf_stack, X_train, y_train, cv=5, scoring=('accuracy'))\n",
    "print('\\n')\n",
    "print('The cross-validated Accuracy of Stacking Model is ', np.mean(scores['test_score']))\n",
    "\n",
    "perf_eval_estimators.append( ('Stacking Classifier', np.mean(scores['test_score']), clf_stack) )\n",
    "\n",
    "perf_eval_estimators.sort(key=lambda i:i[1],reverse=True)\n",
    "final_model = perf_eval_estimators[0][2]\n",
    "final_model_accuracy = perf_eval_estimators[0][1]\n",
    "final_model_name = perf_eval_estimators[0][0]\n",
    "print(\"\\n######## The Final Model selected is: ########\")\n",
    "print(final_model_name)\n",
    "print('The cross-validated Accuracy is: ', final_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested model hparams\n",
    "models_hparams = [\n",
    "    {'solver': ['liblinear', 'saga'], 'penalty': ['l1', 'l2'], 'C': [1e-5, 5e-5, 1e-4, 5e-4, 1], 'fit_intercept':[True, False]},\n",
    "    \n",
    "    {'C': [1e-4, 1e-2, 1, 1e1, 1e2], 'gamma': ['scale', 1e-2, 1e-3, 1e-4, 1e-5], 'kernel': ['linear', 'rbf']},\n",
    "    \n",
    "    {'n_neighbors': list(range(1, 10, 2))},\n",
    "    \n",
    "    {'max_depth': [3, 4, 5, 7, 10], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'hidden_layer_sizes': [(40), (50), (80)], 'max_iter': [100],\n",
    "     'activation': ['logistic', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam'], 'alpha': [1e-4, 1e-2, 1, 1e1, 1e2]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2, 0.3], 'n_estimators': [50, 100, 200], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5, 1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'alpha': [0.1, 1.0, 10.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
    "    \n",
    "    {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'], 'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {},  # GaussianNB has no hyperparameters to tune\n",
    "    \n",
    "    {'radius': [1.0, 1.5, 2.0, 2.5, 3.0], 'weights': ['uniform', 'distance']},\n",
    "    \n",
    "    {'reg_param': [0.0, 0.1, 0.5, 1.0], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': ['auto', None, 0.1, 0.5, 1.0]},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2], 'max_iter': [100, 200], 'max_leaf_nodes': [31, 127, 255], 'max_depth': [None, 3, 5, 7, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
