{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from gensim.models import FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"BoW\" # BoW FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/train.csv')\n",
    "random_state = 42\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "# OCHO MI DA ERRORE PER DEI NAN\n",
    "df = df.dropna(subset=['tweet'])\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Check the mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_mapping)\n",
    "\n",
    "X = df['tweet']\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "if method==\"BoW\":\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_val = vectorizer.transform(X_val)\n",
    "    \n",
    "elif method==\"FastText\":\n",
    "    fasttext_model = FastText(sentences=X_train.apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "    X_train = np.array([np.mean([fasttext_model.wv[word] for word in tweet.split() if word in fasttext_model.wv] or [np.zeros(100)], axis=0) for tweet in X_train])\n",
    "    X_val = np.array([np.mean([fasttext_model.wv[word] for word in tweet.split() if word in fasttext_model.wv] or [np.zeros(100)], axis=0) for tweet in X_val])\n",
    "    \n",
    "elif method==\"GloVe\":\n",
    "    def load_glove_model(glove_file):\n",
    "        model = {}\n",
    "        with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array([float(val) for val in split_line[1:]])\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "    glove_path = '../src/data/glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "    glove_model = load_glove_model(glove_path)\n",
    "\n",
    "    def get_glove_embeddings(tweet, model, embedding_dim=200):\n",
    "        words = tweet.split()\n",
    "        embeddings = [model[word] for word in words if word in model]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(embedding_dim)\n",
    "\n",
    "    X_train = np.array([get_glove_embeddings(tweet, glove_model) for tweet in X_train])\n",
    "    X_val= np.array([get_glove_embeddings(tweet, glove_model) for tweet in X_val])\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Method not allowed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    LinearSVC(random_state=random_state),\n",
    "    #KNeighborsClassifier(n_jobs=-1),\n",
    "    MLPClassifier(verbose=False, random_state=random_state),\n",
    "    RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    AdaBoostClassifier(random_state=random_state),\n",
    "    ExtraTreesClassifier(random_state=random_state),\n",
    "    RidgeClassifier(random_state=random_state),\n",
    "    SGDClassifier(random_state=random_state),\n",
    "    GaussianNB(),\n",
    "    XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss'),\n",
    "    LGBMClassifier(random_state=random_state)\n",
    "]\n",
    "\n",
    "          \n",
    "models_names = [\n",
    "    'Logistic Regression',\n",
    "    'Support Vector Machine',\n",
    "    #'K Nearest Neighbors',\n",
    "    'Multi Layer Perceptron',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'AdaBoost',\n",
    "    'Extra Trees',\n",
    "    'Ridge Classifier',\n",
    "    'SGD Classifier',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'XGBoost',\n",
    "    'LightGBM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_name in zip(models, models_names):\n",
    "    print(f\"\\n########       {model_name}       ########\")\n",
    "    starting_time = time.time()\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Print classification report\n",
    "    #print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    ending_time = time.time()\n",
    "    print(f'Elapsed time: {timedelta(seconds=ending_time - starting_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    LinearSVC(random_state=random_state),\n",
    "    #KNeighborsClassifier(n_jobs=-1),\n",
    "    MLPClassifier(verbose=False, random_state=random_state),\n",
    "    RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    AdaBoostClassifier(random_state=random_state),\n",
    "    ExtraTreesClassifier(random_state=random_state),\n",
    "    RidgeClassifier(random_state=random_state),\n",
    "    SGDClassifier(random_state=random_state),\n",
    "    GaussianNB(),\n",
    "    XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss'),\n",
    "    LGBMClassifier(random_state=random_state)\n",
    "]\n",
    "\n",
    "          \n",
    "models_names = [\n",
    "    'Logistic Regression',\n",
    "    'Support Vector Machine',\n",
    "    #'K Nearest Neighbors',\n",
    "    'Multi Layer Perceptron',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'AdaBoost',\n",
    "    'Extra Trees',\n",
    "    'Ridge Classifier',\n",
    "    'SGD Classifier',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'XGBoost',\n",
    "    'LightGBM'\n",
    "]\n",
    "\n",
    "\n",
    "models_hparams = [\n",
    "    {'solver': ['lbfgs'], 'penalty': ['l2'], 'C': [1e7], 'max_iter':[1000]},\n",
    "    #{'solver': ['saga'], 'penalty': ['l2'], 'C': [9e-2], 'fit_intercept':[True]},\n",
    "    \n",
    "    {'penalty': ['l2'], 'C': [3.75e-2], 'fit_intercept':[True]},\n",
    "    \n",
    "    #{'n_neighbors': [10], 'weights':['uniform']},\n",
    "    \n",
    "    {'hidden_layer_sizes': [(20)], 'max_iter': [100], 'activation': ['relu'], 'solver': ['adam'], 'alpha': [1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2, 0.3], 'n_estimators': [50, 100, 200], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5, 1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'alpha': [0.1, 1.0, 10.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
    "    \n",
    "    {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'], 'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {},  # GaussianNB has no hyperparameters to tune\n",
    "\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'num_leaves': [31, 50, 100], 'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [50, 100, 200], 'max_depth': [-1, 3, 5, 7, 10]}\n",
    "]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''chosen_hparams = list()\n",
    "estimators = list()\n",
    "\n",
    "for model, model_name, hparams in zip(models, models_names, models_hparams):\n",
    "    \n",
    "        print(\"\\n########       {}       ########\".format(model_name))\n",
    "        starting_time = time.time()\n",
    "        clf = GridSearchCV(estimator=model, param_grid=hparams, scoring='accuracy', cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        ending_time = time.time()\n",
    "        chosen_hparams.append(clf.best_params_)\n",
    "        estimators.append((model_name, clf.best_score_, clf.best_estimator_))\n",
    "        \n",
    "        for hparam in hparams:\n",
    "            print(f'\\t--> best value for hyperparameter \"{hparam}\": ', clf.best_params_.get(hparam))\n",
    "        \n",
    "        mean_test_score = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "        std_test_score = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "    \n",
    "        print(f'\\t--> best model mean accuracy:', mean_test_score)\n",
    "        print(f'\\t--> best model std:', std_test_score)\n",
    "        print(f'\\tElapsed time for GridSearch: ', timedelta(seconds=ending_time - starting_time))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ensemble: Stacking Classifier with top 3 weak learners\n",
    "\n",
    "# Sort estimators by the accuracy metric\n",
    "estimators.sort(key=lambda i:i[1],reverse=True)\n",
    "\n",
    "# Get the top 3 classifiers by their accuracy metric\n",
    "top3_clfs = list()\n",
    "for clf in estimators[0:3]:\n",
    "    top3_clfs.append((clf[0], clf[2]))\n",
    "    \n",
    "# Instantiate the Stacking Classifier with the top 3 weak learners\n",
    "clf_stack = StackingClassifier(estimators = top3_clfs, final_estimator = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_eval_estimators = list()\n",
    "for model_tuple in estimators:\n",
    "    model_name = model_tuple[0]\n",
    "    model = model_tuple[2]\n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, scoring=('accuracy'))\n",
    "    print('\\n')\n",
    "    print('The cross-validated Accuracy of {} is: '.format(model_name), np.mean(scores['test_score']))\n",
    "    perf_eval_estimators.append((model_name, np.mean(scores['test_score']), model))\n",
    "\n",
    "# Cross Validation for Stacking Ensemble\n",
    "scores = cross_validate(clf_stack, X_train, y_train, cv=5, scoring=('accuracy'))\n",
    "print('\\n')\n",
    "print('The cross-validated Accuracy of Stacking Model is ', np.mean(scores['test_score']))\n",
    "\n",
    "perf_eval_estimators.append( ('Stacking Classifier', np.mean(scores['test_score']), clf_stack) )\n",
    "\n",
    "perf_eval_estimators.sort(key=lambda i:i[1],reverse=True)\n",
    "final_model = perf_eval_estimators[0][2]\n",
    "final_model_accuracy = perf_eval_estimators[0][1]\n",
    "final_model_name = perf_eval_estimators[0][0]\n",
    "print(\"\\n######## The Final Model selected is: ########\")\n",
    "print(final_model_name)\n",
    "print('The cross-validated Accuracy is: ', final_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested model hparams\n",
    "models_hparams = [\n",
    "    {'solver': ['liblinear', 'saga'], 'penalty': ['l1', 'l2'], 'C': [1e-5, 5e-5, 1e-4, 5e-4, 1], 'fit_intercept':[True, False]},\n",
    "    \n",
    "    {'C': [1e-4, 1e-2, 1, 1e1, 1e2], 'gamma': ['scale', 1e-2, 1e-3, 1e-4, 1e-5], 'kernel': ['linear', 'rbf']},\n",
    "    \n",
    "    {'n_neighbors': list(range(1, 10, 2))},\n",
    "    \n",
    "    {'max_depth': [3, 4, 5, 7, 10], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'hidden_layer_sizes': [(40), (50), (80)], 'max_iter': [100],\n",
    "     'activation': ['logistic', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam'], 'alpha': [1e-4, 1e-2, 1, 1e1, 1e2]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2, 0.3], 'n_estimators': [50, 100, 200], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5, 1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'alpha': [0.1, 1.0, 10.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
    "    \n",
    "    {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'], 'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {},  # GaussianNB has no hyperparameters to tune\n",
    "    \n",
    "    {'radius': [1.0, 1.5, 2.0, 2.5, 3.0], 'weights': ['uniform', 'distance']},\n",
    "    \n",
    "    {'reg_param': [0.0, 0.1, 0.5, 1.0], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': ['auto', None, 0.1, 0.5, 1.0]},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2], 'max_iter': [100, 200], 'max_leaf_nodes': [31, 127, 255], 'max_depth': [None, 3, 5, 7, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
