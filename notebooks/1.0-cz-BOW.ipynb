{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet     label\n",
      "0                  looking forward meeting hope help  positive\n",
      "1  buzzin win even draw like looking moment point...  negative\n",
      "2  dancer novel hardcover acclaimed author side b...  negative\n",
      "3  matter happens want travelling around europe a...  positive\n",
      "4            take vodashop would asking one mxm next  negative\n",
      "{'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/train_small_no_sarcasm.csv')\n",
    "random_state = 42\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "# OCHO MI DA ERRORE PER DEI NAN\n",
    "df = df.dropna(subset=['tweet'])\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Check the mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_mapping)\n",
    "\n",
    "X_train = df['tweet']\n",
    "y_train = df['label']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    #LogisticRegression(fit_intercept=False, random_state=random_state),\n",
    "    #LinearSVC(random_state=random_state),\n",
    "    KNeighborsClassifier(n_jobs=-1),\n",
    "    DecisionTreeClassifier(class_weight='balanced', random_state=random_state),\n",
    "    MLPClassifier(verbose=False, random_state=random_state),\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=random_state),\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    AdaBoostClassifier(random_state=random_state),\n",
    "    ExtraTreesClassifier(class_weight='balanced', random_state=random_state),\n",
    "    RidgeClassifier(class_weight='balanced', random_state=random_state),\n",
    "    SGDClassifier(class_weight='balanced', random_state=random_state),\n",
    "    GaussianNB(),\n",
    "    RadiusNeighborsClassifier(weights='distance'),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    HistGradientBoostingClassifier(random_state=random_state),\n",
    "    XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    LGBMClassifier(random_state=random_state)\n",
    "]\n",
    "\n",
    "          \n",
    "models_names = [\n",
    "    #'Logistic Regression',\n",
    "    #'Support Vector Machine',\n",
    "    'K Nearest Neighbors',\n",
    "    'Decision Tree',\n",
    "    'Multi Layer Perceptron',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'AdaBoost',\n",
    "    'Extra Trees',\n",
    "    'Ridge Classifier',\n",
    "    'SGD Classifier',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'Radius Neighbors',\n",
    "    'Quadratic Discriminant Analysis',\n",
    "    'Linear Discriminant Analysis',\n",
    "    'Histogram-based Gradient Boosting'\n",
    "]\n",
    "\n",
    "models_hparams = [\n",
    "    #{'solver': ['saga'], 'penalty': ['l2'], 'C': [9e-2], 'fit_intercept':[True]},\n",
    "    \n",
    "    #{'penalty': ['l2'], 'C': [3.75e-2], 'fit_intercept':[True]},\n",
    "    \n",
    "    {'n_neighbors': list(range(1, 10, 2)), 'weights':['uniform', 'distance'], 'leaf_size':[30, 50]},\n",
    "    \n",
    "    {'max_depth': [3, 4, 5, 7, 10], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'hidden_layer_sizes': [(40), (50), (80)], 'max_iter': [100, 200],\n",
    "     'activation': ['logistic', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam'], 'alpha': [1e-4, 1e-2, 1, 1e1, 1e2]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2, 0.3], 'n_estimators': [50, 100, 200], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5, 1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'alpha': [0.1, 1.0, 10.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
    "    \n",
    "    {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'], 'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {},  # GaussianNB has no hyperparameters to tune\n",
    "    \n",
    "    {'radius': [1.0, 1.5, 2.0, 2.5, 3.0], 'weights': ['uniform', 'distance']},\n",
    "    \n",
    "    {'reg_param': [0.0, 0.1, 0.5, 1.0], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': ['auto', None, 0.1, 0.5, 1.0]},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2], 'max_iter': [100, 200], 'max_leaf_nodes': [31, 127, 255], 'max_depth': [None, 3, 5, 7, 10]}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########       K Nearest Neighbors       ########\n"
     ]
    }
   ],
   "source": [
    "chosen_hparams = list()\n",
    "estimators = list()\n",
    "\n",
    "for model, model_name, hparams in zip(models, models_names, models_hparams):\n",
    "    \n",
    "        print(\"\\n########       {}       ########\".format(model_name))\n",
    "        starting_time = time.time()\n",
    "        clf = GridSearchCV(estimator=model, param_grid=hparams, scoring='accuracy', cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        ending_time = time.time()\n",
    "        chosen_hparams.append(clf.best_params_)\n",
    "        estimators.append((model_name, clf.best_score_, clf.best_estimator_))\n",
    "        \n",
    "        for hparam in hparams:\n",
    "            print(f'\\t--> best value for hyperparameter \"{hparam}\": ', clf.best_params_.get(hparam))\n",
    "            \n",
    "        print(f'\\t--> accuracy: ', clf.best_score_)\n",
    "        print(f'\\tElapsed time for GridSearch: ', timedelta(seconds=ending_time - starting_time))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ensemble: Stacking Classifier with top 3 weak learners\n",
    "\n",
    "# Sort estimators by the balanced accuracy metric\n",
    "estimators.sort(key=lambda i:i[1],reverse=True)\n",
    "\n",
    "# Get the top 3 classifiers by their accuracy metric\n",
    "top3_clfs = list()\n",
    "for clf in estimators[0:3]:\n",
    "    top3_clfs.append((clf[0], clf[2]))\n",
    "    \n",
    "# Instantiate the Stacking Classifier with the top 3 weak learners\n",
    "clf_stack = StackingClassifier(estimators = top3_clfs, final_estimator = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_eval_estimators = list()\n",
    "for model_tuple in estimators:\n",
    "    model_name = model_tuple[0]\n",
    "    model = model_tuple[2]\n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, scoring=('accuracy'))\n",
    "    print('\\n')\n",
    "    print('The cross-validated Accuracy of {} is: '.format(model_name), np.mean(scores['test_score']))\n",
    "    perf_eval_estimators.append((model_name, np.mean(scores['test_score']), model))\n",
    "\n",
    "# Cross Validation for Stacking Ensemble\n",
    "scores = cross_validate(clf_stack, X_train, y_train, cv=5, scoring=('accuracy'))\n",
    "print('\\n')\n",
    "print('The cross-validated Accuracy of Stacking Model is ', np.mean(scores['test_score']))\n",
    "\n",
    "perf_eval_estimators.append( ('Stacking Classifier', np.mean(scores['test_score']), clf_stack) )\n",
    "\n",
    "perf_eval_estimators.sort(key=lambda i:i[1],reverse=True)\n",
    "final_model = perf_eval_estimators[0][2]\n",
    "final_model_accuracy = perf_eval_estimators[0][1]\n",
    "final_model_name = perf_eval_estimators[0][0]\n",
    "print(\"\\n######## The Final Model selected is: ########\")\n",
    "print(final_model_name)\n",
    "print('The cross-validated Accuracy is: ', final_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested model hparams\n",
    "models_hparams = [\n",
    "    {'solver': ['liblinear', 'saga'], 'penalty': ['l1', 'l2'], 'C': [1e-5, 5e-5, 1e-4, 5e-4, 1], 'fit_intercept':[True, False]},\n",
    "    \n",
    "    {'C': [1e-4, 1e-2, 1, 1e1, 1e2], 'gamma': ['scale', 1e-2, 1e-3, 1e-4, 1e-5], 'kernel': ['linear', 'rbf']},\n",
    "    \n",
    "    {'n_neighbors': list(range(1, 10, 2))},\n",
    "    \n",
    "    {'max_depth': [3, 4, 5, 7, 10], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'hidden_layer_sizes': [(40), (50), (80)], 'max_iter': [100, 200],\n",
    "     'activation': ['logistic', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam'], 'alpha': [1e-4, 1e-2, 1, 1e1, 1e2]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2, 0.3], 'n_estimators': [50, 100, 200], 'max_depth': [3, 4, 5, 7, 10]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5, 1]},\n",
    "    \n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']},\n",
    "    \n",
    "    {'alpha': [0.1, 1.0, 10.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
    "    \n",
    "    {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'], 'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {},  # GaussianNB has no hyperparameters to tune\n",
    "    \n",
    "    {'radius': [1.0, 1.5, 2.0, 2.5, 3.0], 'weights': ['uniform', 'distance']},\n",
    "    \n",
    "    {'reg_param': [0.0, 0.1, 0.5, 1.0], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    \n",
    "    {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': ['auto', None, 0.1, 0.5, 1.0]},\n",
    "    \n",
    "    {'learning_rate': [0.01, 0.1, 0.2], 'max_iter': [100, 200], 'max_leaf_nodes': [31, 127, 255], 'max_depth': [None, 3, 5, 7, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
