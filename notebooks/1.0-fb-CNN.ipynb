{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 - load training data (FULL)\n",
    "train_df = pd.read_csv('../data/processed/train_full.csv')\n",
    "train_df = train_df.dropna(subset=['tweet'])\n",
    "X = train_df['tweet']\n",
    "y = train_df['label']\n",
    "\n",
    "# 1.2 - split dataset into training, validation, and test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - TOKENIZATION + PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 - tokenization\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequence_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequence_val = tokenizer.texts_to_sequences(X_validation)\n",
    "sequence_test = tokenizer.texts_to_sequences(X_test)\n",
    "word2vec = tokenizer.word_index\n",
    "V = len(word2vec)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 2.2 - padding\n",
    "data_train = pad_sequences(sequence_train)\n",
    "T = data_train.shape[1]\n",
    "data_val = pad_sequences(sequence_val, maxlen=T)\n",
    "data_test = pad_sequences(sequence_test, maxlen=T)\n",
    "\n",
    "# 2.3 - convert to PyTorch tensors\n",
    "X_train = torch.tensor(data_train, dtype=torch.long)\n",
    "y_train = torch.tensor(LabelEncoder().fit_transform(y_train), dtype=torch.float32)\n",
    "X_val = torch.tensor(data_val, dtype=torch.long)\n",
    "y_val = torch.tensor(LabelEncoder().fit_transform(y_validation), dtype=torch.float32)\n",
    "X_test = torch.tensor(data_test, dtype=torch.long)\n",
    "y_test = torch.tensor(LabelEncoder().fit_transform(y_test), dtype=torch.float32)\n",
    "\n",
    "# 2.4 - create dataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.0 - model (v2)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.global_pool(F.relu(self.conv3(x))).squeeze(2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.1 - initialize model\n",
    "model = CNN(vocab_size=V+1, embed_dim=20)\n",
    "# 3.1.2 - define loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# 3.1.3- define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4096545150249858, Train Accuracy: 80.74087002268898%, Validation Loss: 0.37800456500143437, Validation Accuracy: 82.56773192081806%\n",
      "Epoch 2, Train Loss: 0.370668632401087, Train Accuracy: 83.22270558483014%, Validation Loss: 0.3640604670709347, Validation Accuracy: 83.32826577477636%\n",
      "Epoch 3, Train Loss: 0.35998309602590445, Train Accuracy: 83.86466824927774%, Validation Loss: 0.35967883894184705, Validation Accuracy: 83.71113315943606%\n",
      "Epoch 4, Train Loss: 0.3530455173528429, Train Accuracy: 84.28144139296447%, Validation Loss: 0.3569756394878785, Validation Accuracy: 83.74273872201508%\n",
      "Epoch 5, Train Loss: 0.3476858452766666, Train Accuracy: 84.57804344662489%, Validation Loss: 0.354937363349195, Validation Accuracy: 84.03118948935013%\n",
      "Epoch 6, Train Loss: 0.3438688021187192, Train Accuracy: 84.80278288839692%, Validation Loss: 0.35532579461519864, Validation Accuracy: 84.11440413512778%\n",
      "Epoch 7, Train Loss: 0.34099982641178295, Train Accuracy: 85.00526842460852%, Validation Loss: 0.35369929181067566, Validation Accuracy: 84.14200899358288%\n",
      "Epoch 8, Train Loss: 0.33846028492762, Train Accuracy: 85.1190884000142%, Validation Loss: 0.35997169343416074, Validation Accuracy: 84.07279681223895%\n",
      "Epoch 9, Train Loss: 0.33632853037364285, Train Accuracy: 85.26776449266846%, Validation Loss: 0.3563463966807096, Validation Accuracy: 83.99278272976044%\n",
      "Epoch 10, Train Loss: 0.335029045095261, Train Accuracy: 85.38548515264429%, Validation Loss: 0.3548415178691523, Validation Accuracy: 84.25642913152714%\n",
      "Epoch 11, Train Loss: 0.334898610756333, Train Accuracy: 85.44249515790021%, Validation Loss: 0.35072136197870535, Validation Accuracy: 84.17641504904863%\n",
      "Epoch 12, Train Loss: 0.3326541427756585, Train Accuracy: 85.52405947243741%, Validation Loss: 0.35845319694837985, Validation Accuracy: 84.13400758533501%\n",
      "Epoch 13, Train Loss: 0.3332404337077021, Train Accuracy: 85.6293279470547%, Validation Loss: 0.35204478226319774, Validation Accuracy: 84.16801357038838%\n",
      "Epoch 14, Train Loss: 0.3304860088232041, Train Accuracy: 85.67903667093574%, Validation Loss: 0.35235602024959806, Validation Accuracy: 83.99518315223479%\n",
      "Epoch 15, Train Loss: 0.32927041380374233, Train Accuracy: 85.74579838761703%, Validation Loss: 0.36775748464872576, Validation Accuracy: 83.85515850789739%\n",
      "Epoch 16, Train Loss: 0.33032363790343294, Train Accuracy: 85.77645376763623%, Validation Loss: 0.3613355435865907, Validation Accuracy: 83.98318103986301%\n",
      "----> early stopped\n"
     ]
    }
   ],
   "source": [
    "# 3.2.0 - since overfitting was observed -> early stopping parameters\n",
    "wait = 5\n",
    "best_val_loss = np.inf\n",
    "epochs_no_impr = 0\n",
    "early_stop = False\n",
    "\n",
    "# 3.2.1 - training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    # 3.2.2 - validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = model(texts).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader)}, Train Accuracy: {train_accuracy}%, '\n",
    "          f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "\n",
    "    # 3.2.3 - early stopping \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_impr = 0\n",
    "        torch.save(model.state_dict(), 'best_CNN_model.pt')\n",
    "    else:\n",
    "        epochs_no_impr += 1\n",
    "        if epochs_no_impr >= wait:\n",
    "            print('----> early stopped')\n",
    "            early_stop = True\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.31956296302026455, Test Accuracy: 85.94333305495114%\n"
     ]
    }
   ],
   "source": [
    "# 3.3.0 - load best model\n",
    "model.load_state_dict(torch.load('best_CNN_model.pt'))\n",
    "\n",
    "# 3.3.1 - testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in train_loader:\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f'Test Loss: {test_loss / len(train_loader)}, Test Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.0 - model (v3)\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.lstm = nn.LSTM(128, lstm_hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.global_pool(F.relu(self.conv3(x))).squeeze(2).unsqueeze(1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.1 - initialize model\n",
    "model = CNN_LSTM(vocab_size=V+1, embed_dim=20, lstm_hidden_dim=128, num_classes=1)\n",
    "# 4.1.2 - loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# 4.1.3 - optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.40722891923143284, Train Accuracy: 80.90429870442263%, Validation Loss: 0.38796472878477173, Validation Accuracy: 82.18886524028228%\n",
      "Epoch 2, Train Loss: 0.3695256621184631, Train Accuracy: 83.26476296590052%, Validation Loss: 0.3656213274897976, Validation Accuracy: 83.3150634511674%\n",
      "Epoch 3, Train Loss: 0.36238822561832096, Train Accuracy: 83.7508982826486%, Validation Loss: 0.36247427750586764, Validation Accuracy: 83.41148042055401%\n",
      "Epoch 4, Train Loss: 0.3621384003599201, Train Accuracy: 83.85841715221021%, Validation Loss: 0.3660665560080739, Validation Accuracy: 83.38867640704764%\n",
      "Epoch 5, Train Loss: 0.3741848522914965, Train Accuracy: 83.35482877244957%, Validation Loss: 0.3900517421487015, Validation Accuracy: 81.76879130727008%\n",
      "Epoch 6, Train Loss: 0.3801707312753247, Train Accuracy: 83.16184490378062%, Validation Loss: 0.38448392930385766, Validation Accuracy: 83.02181183888365%\n",
      "Epoch 7, Train Loss: 0.40574306515199715, Train Accuracy: 82.28664130554913%, Validation Loss: 0.4001527686106948, Validation Accuracy: 82.40770375586104%\n",
      "Epoch 8, Train Loss: 0.3979284955545423, Train Accuracy: 82.61549902007802%, Validation Loss: 0.40414916605703416, Validation Accuracy: 82.28608235049369%\n",
      "----> early stopped\n"
     ]
    }
   ],
   "source": [
    "# 4.2.0 - since overfitting was observed -> early stopping parameters\n",
    "wait = 5\n",
    "best_val_loss = np.inf\n",
    "epochs_no_impr = 0\n",
    "early_stop = False\n",
    "# 4.2.1 - training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    # 4.2.2 - validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = model(texts).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader)}, Train Accuracy: {train_accuracy}%, '\n",
    "          f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "\n",
    "    # 4.2.3 - early stopping \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_impr = 0\n",
    "        torch.save(model.state_dict(), 'best_CNN_LSTM_model.pt')\n",
    "    else:\n",
    "        epochs_no_impr += 1\n",
    "        if epochs_no_impr >= wait:\n",
    "            print('----> early stopped')\n",
    "            early_stop = True\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3526623520912514, Test Accuracy: 84.02204586905002%\n"
     ]
    }
   ],
   "source": [
    "# 4.3.0 - load best model\n",
    "model.load_state_dict(torch.load('best_CNN_LSTM_model.pt'))\n",
    "\n",
    "# 4.3.1 - testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in train_loader:\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f'Test Loss: {test_loss / len(train_loader)}, Test Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - LSTM-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.0 - model (v3)\n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, num_classes):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True)\n",
    "        self.conv1 = nn.Conv1d(in_channels=lstm_hidden_dim, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.global_pool(F.relu(self.conv3(x))).squeeze(2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.1 - initialize model\n",
    "model = LSTM_CNN(vocab_size=V+1, embed_dim=20, lstm_hidden_dim=128, num_classes=1)\n",
    "# 5.1.2- loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# 5.1.3 - optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.3912068454026794, Train Accuracy: 81.89592273443989%, Validation Loss: 0.3576830866895864, Validation Accuracy: 83.80434956552354%\n",
      "Epoch 2, Train Loss: 0.34587015811801614, Train Accuracy: 84.62010082769525%, Validation Loss: 0.3423587308535653, Validation Accuracy: 84.68530461361199%\n",
      "Epoch 3, Train Loss: 0.33197585107717803, Train Accuracy: 85.33257586706468%, Validation Loss: 0.33768707914854923, Validation Accuracy: 84.81692777928916%\n",
      "Epoch 4, Train Loss: 0.3235973163206976, Train Accuracy: 85.78415511922343%, Validation Loss: 0.3374705841474872, Validation Accuracy: 85.03296580198115%\n",
      "Epoch 5, Train Loss: 0.31804365098092197, Train Accuracy: 86.11491316726085%, Validation Loss: 0.3338499033811211, Validation Accuracy: 85.08737537806654%\n",
      "Epoch 6, Train Loss: 0.3132859758202003, Train Accuracy: 86.34105285477601%, Validation Loss: 0.3383933071302669, Validation Accuracy: 84.98935812703036%\n",
      "Epoch 7, Train Loss: 0.30986553956688184, Train Accuracy: 86.51373316016961%, Validation Loss: 0.3414349664456635, Validation Accuracy: 84.85293411640448%\n",
      "Epoch 8, Train Loss: 0.30772505503061104, Train Accuracy: 86.63125378503928%, Validation Loss: 0.34945603620777876, Validation Accuracy: 84.72411144361408%\n",
      "Epoch 9, Train Loss: 0.30618110524070496, Train Accuracy: 86.73687232109235%, Validation Loss: 0.3385543061140877, Validation Accuracy: 84.84773320104338%\n",
      "Epoch 10, Train Loss: 0.304638966893371, Train Accuracy: 86.82568790822789%, Validation Loss: 0.33841207714038923, Validation Accuracy: 84.93934932548129%\n",
      "----> early stopped\n"
     ]
    }
   ],
   "source": [
    "# 5.2.0 - since overfitting was observed -> early stopping parameters\n",
    "wait = 5\n",
    "best_val_loss = np.inf\n",
    "epochs_no_impr = 0\n",
    "early_stop = False\n",
    "\n",
    "# 5.2.1 - training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    # 5.2.2 - validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = model(texts).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader)}, Train Accuracy: {train_accuracy}%, '\n",
    "          f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "\n",
    "    # 5.2.3 - early stopping \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_impr = 0\n",
    "        torch.save(model.state_dict(), 'best_LSTM_CNN_model.pt')\n",
    "    else:\n",
    "        epochs_no_impr += 1\n",
    "        if epochs_no_impr >= wait:\n",
    "            print('----> early stopped')\n",
    "            early_stop = True\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.302466902563521, Test Accuracy: 86.78243031652055%\n"
     ]
    }
   ],
   "source": [
    "# 5.3.0 - load best model\n",
    "model.load_state_dict(torch.load('best_LSTM_CNN_model.pt'))\n",
    "\n",
    "# 5.3.1 - testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in train_loader:\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f'Test Loss: {test_loss / len(train_loader)}, Test Accuracy: {test_accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
